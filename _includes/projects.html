<table class="projects">
  <tbody>
      <!-- ! ROB 550 Botlab -->
      <tr>
        <td>
          <video src="assets/img/Kinapped_Cat.mp4" width="250" controls="controls" muted="muted"></video>
        </td>
        <td valign="top" align="left">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Robotics System Lab for MBot Autonomy</b></font>
          </p>
          <p>
            [<a href="https://github.com/silvery107/silvery-botlab-f22" target="_blank">
              Code
            </a>]
            [<a href="https://github.com/silvery107/silvery-botlab-f22/blob/main/data/S2T1_Botlab_Report.pdf" target="_blank">
              PDF
            </a>]
            <br>
            In the Botlab, movement control, obstacle detection, maze exploration, and self-localization functionality was developed on the MBot robot, a mobile robot platform.
            It is designed to explore the fundamentals of robot autonomy by developing MBot with autonomous mapping, localization, and exploration capabilities. 
          </p>
        </td>
      </tr>
    <!-- ! ROB 550 Armlab -->
    <tr>
      <td>
        <video src="assets/img/stack_16_high.mp4" height="190" controls="controls" muted="muted"></video>
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Robotics System Lab for Robot Arm</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/silvery-armlab-f22" target="_blank">
            Code
          </a>]
          [<a href="assets/img/block_detection.png" target="_blank">
            Block Detections
          </a>]
          <br>
          In the Armlab, a 5-DOF robotic arm fully autonomously arranges blocks of different sizes, colors and positions into the desired arrangement. Analytical inverse kinematics is used to determine the appropriate waypoints. An overhead LiDAR Camera is utilized to identify blocks on the board.
        </p>
      </td>
    </tr>
    <!-- ! Deep RL for MPC Control of Quadruped Locomotion -->
    <tr>
      <td>
        <img src="assets/img/MPC_Stair_Demo.gif" width="250" class="zoom">
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Deep RL for MPC of Quadruped Locomotion</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/rl-mpc-locomotion" target="_blank">
            Code
          </a>]
          [<a href="assets/img/MPC_Sim2Real.gif" target="_blank">
            Sim2Real
          </a>]
          [<a href="assets/img/RL_Paraller_16.gif" target="_blank">
            Parallel Training
          </a>]
          <br>
          Transferring MIT Cheetah controller to NVIDIA Isaac Gym to control Aliengo quadruped robot.
          Building neural network policy and using PPO algorithm to train parameters of the controller.
        </p>
      </td>
    </tr>
    <!-- ! Mobile Robot Navigation and Control Capstone-->
    <tr>
      <td>
        <video src="assets/img/ee346-demo.mp4" width="250" controls="controls" muted="muted"></video>
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Mobile Robot Navigation and Control Capstone</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/ee346-capstone-control" target="_blank">
            Code
          </a>]
          [<a href="https://sites.google.com/view/ee346-capstone-22s-cmdzyl/home" target="_blank">
            Website
          </a>]
          <br>
          We achieved a multifunctional and integrated system that includes autonomous navigation, 
          lane following and Aruco detection using TurtleBot3 Burger. 
          Our system is designed for the competition of finishing a specified task sequence.
        </p>
      </td>
    </tr>
    <!-- ! Agile Waste Sorting with Tossing -->
    <tr>
      <td>
        <video src="assets/img/tossing-video-4-3.mp4" width="250" controls="controls" muted="muted"></video>
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Agile Waste Sorting with Tossing</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/ME336-Yellow-Team-Project" target="_blank">
            Code
          </a>]
          [<a href="https://github.com/silvery107/ME336-Yellow-Team-Project/blob/main/projects/ME336_Report.pdf" target="_blank">
            PDF
          </a>]
          <br>
          Implemented the automatic collection and cleaning of images based on MOG2 algorithm.
          Deployed and trained YOLOv5 to achieve quick waste classification.
          Achieved planning for robot arm to pick toss waste on dynamic conveyor belt.
        </p>
      </td>
    </tr>
    <!-- ! NMT with Transformer on Multi30K -->
    <tr>
      <td>
        <img src="https://raw.githubusercontent.com/silvery107/nmt-multi30k-pytorch/main/images/tuning.png" 
          width="250" class="zoom">
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>NMT with Transformer on Multi30K</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/nmt-multi30k-pytorch" target="_blank">
            Code
          </a>]
          [<a href="https://github.com/silvery107/nmt-multi30k-pytorch/blob/main/docs/Machine_Learning_Project_NMT.pdf" target="_blank">
            PDF
          </a>]
          <br>
          Implemented a Transformer architecture to realize a full attention neural network that learns to
          translate German to English. The best model gains a BLEU score up to 37.39, when the minimum
          frequency of words is selected to be 3.
        </p>
      </td>
    </tr>
    <!-- ! Tractor Autonomous Steering Simulation -->
    <!-- <tr>
      <td>
        <img src="https://raw.githubusercontent.com/silvery107/ND-agjunction-webots/main/docs/images/tractor_proto.png"
          width="250" class="zoom">
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Tractor Autonomous Steering Simulation</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/ND-agjunction-webots" target="_blank">
            Code
          </a>]
          [<a href="assets/img/tractor_demo.mp4" target="_blank">
            Video
          </a>]
          <br>
          Developed a medium-fidelity model of two tractors in Webots and implemented the tractor model and
          the control
          algorithm in Webots, including four-wheel steering capability, body suspension and sensors.
          This project is collaborated with SUSTech, ND and AgJunction Inc.
        </p>
      </td>
    </tr> -->
    <!-- ! Segway Locomotion -->
    <!-- <tr>
      <td>
        <img src="https://raw.githubusercontent.com/silvery107/segway-locomotion-stm32/main/images/image2.png"
          width="250" class="zoom">
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Segway Locomotion</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/segway-locomotion-stm32" target="_blank">
            Code
          </a>]
          [<a href="assets/img/blancing.mp4" target="_blank">
            Video
          </a>]
          <br>
          This project is the locomotion control for a Segway robot with STM32F1.
          We implemented balance control, speed control, steering control method for the Segway robot
          together with the Bluetooth debugging function.
        </p>
      </td>
    </tr> -->
    <!-- ! HCI Gesture Control -->
    <tr>
      <td>
        <video src="assets/img/hci-demo-thin.mp4" width="250" controls="controls" muted="muted"></video>
        <!-- <img src=https://raw.githubusercontent.com/silvery107/hci-gesture-control/main/images/demo.png width="250"> -->
      </td>
      <td valign="top" align="left">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>HCI Gesture Control</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/hci-gesture-control" target="_blank">
            Code
          </a>]
          <!-- [<a href="assets/img/hci-demo-thin.mp4" target="_blank">
            Video
          </a>] -->
          <br>
          Used the watershed algorithm to realize skin color image segmentation, implemented gesture
          recognition
          by template matching, and implemented the gesture interaction with mouse and keyboard based on
          <em>Win32
            API</em>.
        </p>
      </td>
    </tr>
    <!-- ! Fast Photo Mosaic -->
    <tr>
      <td style="border: none;">
        <img src="https://github.com/silvery107/fast-photo-mosaic/raw/main/images/mixed.png" 
          width="250" class="zoom">
      </td>
      <td valign="top" align="left" style="border: none;">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Fast Photo Mosaic</b></font>
        </p>
        <p>
          [<a href="https://github.com/silvery107/fast-photo-mosaic" target="_blank">
            Code
          </a>]
          <br>
          Designed a feature descriptor based on the mean histogram of the LAB color space, used the
          pre-computed feature pool to optimize the synthesis speed, and realized the mosaic photo that has
          better performance than Foto-Mosaik-Edda.
        </p>
      </td>
    </tr>
  </tbody>
</table>
